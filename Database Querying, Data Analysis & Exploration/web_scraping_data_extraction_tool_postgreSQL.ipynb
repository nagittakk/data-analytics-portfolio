{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Web Scraping and Data Extraction Tool with PostgreSQL Integration\n\n## Introduction\n\nThis Jupyter Notebook extends the web scraping tool by incorporating a database-driven approach. In addition to crawling web pages and saving HTML content, the tool now also extracts relevant data (e.g., titles, authors, categories, summaries) from each webpage and stores this data in a PostgreSQL database. The database is structured to handle information about articles, their authors, and categories, ensuring efficient data management and retrieval.\n\n### Objectives\n\n- **Crawl Web Pages**: Start from an initial URL and navigate through discovered links using a breadth-first search (BFS) approach.\n- **Download HTML Content**: Save the HTML content of crawled pages into files for later use.\n- **Extract Data**: Parse the HTML content to extract key information like article title, author, category, and summary.\n- **Save Data to Database**: Store the extracted data in a PostgreSQL database using pre-defined tables for articles, authors, and categories.\n- **Parallel Processing**: Use threading and parallel processing to efficiently crawl and extract data from multiple URLs simultaneously.\n\n### PostgreSQL Integration\n\nThe tool creates a PostgreSQL database and sets up tables required to store the extracted data:\n- **Author Table**: Stores information about the authors of articles.\n- **Category Table**: Stores the categories to which each article belongs.\n- **Article Table**: Stores article data, linking each article to its respective author and category via foreign key constraints.\n\n### Dependencies\n\nTo run this notebook, ensure that you have the following libraries installed:\n- `selenium`\n- `BeautifulSoup`\n- `psycopg2` (for interacting with PostgreSQL)\n- `joblib`\n- `threading`\n- `os`\n- `sqlalchemy` (for creating connection pools)\n\n### Database Setup\n\nBefore starting the web crawling and data extraction process, the script:\n1. **Connection Pool**: Utilizes a connection pool to efficiently handle multiple database connections during parallel processing.\n2. **Creates the PostgreSQL Database**: Initializes a PostgreSQL database (if it doesnâ€™t already exist).\n3. **Creates the Necessary Tables**: Sets up tables for storing article data, author information, and categories.\n\n### Data Extraction and Saving\n\nFor each webpage crawled, the tool extracts key information such as the title, author, category, and summary. This data is then saved in the PostgreSQL database.\n\n### Usage\n\nTo start the web crawling and data extraction process:\n1. Set up your PostgreSQL database credentials.\n2. Modify the `start_url`, `MAX_PAGES`, and `NUM_WORKERS` variables as needed.\n3. Execute the cells sequentially to define the necessary functions and initiate the crawling, data extraction, and saving process.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "##### Imported libraries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Standard library imports\nimport os  # For file system operations\nimport threading  # For thread-safe operations on shared variables\n\n# Third-party imports\nimport psycopg2  # PostgreSQL database adapter\nfrom psycopg2 import pool  # Establish global connection to Postgres database\nfrom bs4 import BeautifulSoup  # For parsing HTML content\nfrom joblib import Parallel, delayed  # For parallel processing\nfrom selenium import webdriver  # For sending requests to URLs\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "##### Initalise variables",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Define and initialise variables\n\n# Constants\nMAX_PAGES = 100 # Maximum number of pages to crawl\nNUM_WORKERS = 50 # Number of parallel workers - if default number of 5 workers is not used\n\n# Configuration Variables\nstart_url = \"https://www.news24.com/\" # Initial URL to start crawling from \nfolder = 'crawled_webpages' # Folder to save crawled pages\ndata_batch = []\nsql_scripts = {'Author':'Author_Table.sql', \n               'Category':'Category_Table.sql',\n               'Article':'Article_Table.sql'\n              } # SQL scripts to be called for table creation\n\nurl_start = ['https://www.news24.com/news24',\n             'https://www.news24.com/fin24',\n             'https://www.news24.com/sport',\n             'https://www.news24.com/news24/investigations',\n             'https://www.news24.com/news24/politics',\n             'https://www.news24.com/news24/opinions',\n             'https://www.news24.com/life',\n             'https://www.news24.com/fin24/consumer-lookout',\n             'https://www.news24.com/fin24/climate_future'\n             ] # All possible beginnings of news24 webpages - according to website home page tabs\n               # Ensure that only news24 articles are crawled to extract data conveniently \n               # (uniform data tag convention)\n\n# Runtime Variables\nurls_to_crawl = [start_url]  # Initialise a list of URLs to crawl\ncrawled_count = 0  # Counter to track the number of pages crawled\ncrawled_lock = threading.Lock()  # Lock for safely incrementing the counter\n\n# Postgres server connection credentials\nminconn = 1\nmaxconn = 5\nhost = \"localhost\" # Hostname of the PostgreSQL server\nport = \"5432\" # Port number on which the PostgreSQL server is listening\nuser = \"postgres\" # Username to authenticate with the PostgreSQL server\npassword = \"postgres1\" # Password corresponding to Username\ndb_name = \"section2_db\" # Database name",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "##### Defined functions",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def create_connection_pool(minconn, maxconn, host, port, user, password, database):\n    \"\"\"\n    Create a connection pool for connecting to a specific PostgreSQL database.\n\n    This function initializes a connection pool that can manage multiple connections \n    to the specified PostgreSQL database, allowing for efficient resource usage \n    and improved performance in applications that require frequent database access.\n\n    Parameters:\n        minconn (int): The minimum number of connections to maintain in the pool.\n        maxconn (int): The maximum number of connections allowed in the pool.\n        host (str): The hostname or IP address of the PostgreSQL server.\n        port (int): The port number on which the PostgreSQL server is listening.\n        user (str): The username used to authenticate with the PostgreSQL server.\n        password (str): The password corresponding to the provided username.\n        database (str): The name of the database to which the connections will be made.\n\n    Returns:\n        connection_pool: A connection pool object for the specified database if successful, \n                         or None if an error occurs during pool creation.\n    \n    Raises:\n        Exception: Any exception raised during the creation of the connection pool.\n    \"\"\"\n    try:\n        # Create a connection pool for the specified database\n        connection_pool = psycopg2.pool.SimpleConnectionPool(\n            minconn,\n            maxconn,\n            host=host,\n            port=port,\n            user=user,\n            password=password,\n            database=database \n        )\n        \n        print(\"Connection pool to PostgreSQL server created successfully.\")\n        return connection_pool  # Return the connection pool object\n\n    except Exception as e:\n        print(f\"Error creating connection pool: {e}\")\n        return None  # Return None if the connection pool creation fails\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def add_data_to_tables(connection_pool, data_batch):\n    \"\"\"Add data from a batch of crawled webpages to the PostgreSQL database.\n\n    Args:\n        data_batch (list): A list of data rows, where each row is a tuple containing\n            title, list of author names, date, list of category names, summary, and URL.\n\n    Raises:\n        Exception: If any error occurs during data insertion.\n    \"\"\"\n    global db_name\n    \n    conn = None\n    cur = None\n    \n    try:\n        print(f\"Saving collected data to the database '{db_name}'...\")\n        # Get a connection from the connection pool\n        conn = connection_pool.getconn()\n        cur = conn.cursor()\n\n        for row in data_batch:\n            title, authors, publication_date, categories, summary, url = row\n            \n            author_ids = []\n            # Insert into author table\n            for author in authors:\n                cur.execute(\n                    \"INSERT INTO Author (name) VALUES (%s) ON CONFLICT (name) DO NOTHING RETURNING id\",\n                    (author,)\n                )\n                author_id = cur.fetchone()[0] if cur.rowcount > 0 else None\n                if author_id:\n                    author_ids.append(author_id)\n\n            category_ids = []\n            # Insert into category table\n            for category in categories:\n                cur.execute(\n                    \"INSERT INTO Category (name) VALUES (%s) ON CONFLICT (name) DO NOTHING RETURNING id\",\n                    (category,)\n                )\n                category_id = cur.fetchone()[0] if cur.rowcount > 0 else None\n                if category_id:\n                    category_ids.append(category_id)\n\n            # Insert into article table\n            # Assuming you want to link to the first author and category for simplicity\n            # Adjust this logic as needed based on your requirements\n            author_id = author_ids[0] if author_ids else None\n            category_id = category_ids[0] if category_ids else None\n\n            cur.execute(\n                \"\"\"\n                INSERT INTO Article (title, author_id, publication_date, category_id, summary, url)\n                VALUES (%s, %s, %s, %s, %s, %s)\n                \"\"\",\n                (title, author_id, publication_date, category_id, summary, url)\n            )\n\n        conn.commit()\n\n    except Exception as e:\n        # If any error occurs, rollback the transaction to avoid partial insertion\n        print(f\"Error inserting data: {e}\")\n        if conn:\n            conn.rollback()\n\n    finally:\n        # Close the cursor \n        if cur:\n            cur.close()\n\n        # Release the connection back to the pool\n        if conn:\n            connection_pool.putconn(conn)\n\n        print(f\"Data saving operation completed! All records have been successfully stored in the database '{db_name}'.\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def extract_webpage_data(soup):\n    \"\"\"Extract data from a webpage using BeautifulSoup.\n\n    Args:\n        soup (BeautifulSoup): A BeautifulSoup object representing the parsed HTML of the webpage.\n\n    Returns:\n        list: A list containing the extracted title, authors, date, categories, and summaries.\n    \"\"\"\n    # Extract the title\n    title_tag = soup.find('h1', class_='article__title')\n    title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n\n    # Extract the authors - as a list\n    author_tag = soup.find('div', class_='article__author')\n    authors_text = author_tag.get_text(strip=True) if author_tag else \"N/A\"\n\n    # Initialize an empty list for authors\n    authors = []\n\n    if authors_text != \"N/A\":\n        # Remove \"written by\" phrase if present\n        authors_text = authors_text.replace('written by', '').strip()\n\n        # Split the authors by \"and\" and commas, then strip extra whitespace\n        authors = [author.strip() for author in authors_text.replace('and', ',').split(',')]\n    else:\n        authors = [\"N/A\"]\n\n    # Extract the date\n    date_tag = soup.find('p', class_='article__date') \n    date = date_tag.get_text(strip=True) if date_tag else \"N/A\"\n\n    # Extract the categories - as a list\n    category_tags = soup.find_all('a', attrs={'data-tag': True})  \n    categories = [category_tag.get_text() for category_tag in category_tags] if category_tags else [\"N/A\"]\n\n    # Extract summaries\n    summary_tags = soup.find_all('strong')  # Finds all <strong> tags (assuming each point is in <strong>)\n    summaries = \" \".join([tag.get_text(strip=True) for tag in summary_tags]) if summary_tags else \"N/A\"\n\n    # List of data collected from the webpage\n    data = [title, authors, date, categories, summaries]\n\n    return data\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def create_tables(connection_pool, sql_scripts):\n    \"\"\"Create tables in the PostgreSQL database using provided SQL scripts.\n\n    Args:\n        sql_scripts (dict): A dictionary where keys are table names and values are SQL script file names to execute for table creation.\n\n    Prints:\n        A success message for each script executed, or an error message if any errors occur during execution.\n    \"\"\"\n    conn = None\n    cur = None\n    \n    try:\n        print(\"Creating database tables...\")\n        \n        # Get a connection from the connection pool\n        conn = connection_pool.getconn()\n        \n        # Enable autocommit mode to allow CREATE TABLE commands to execute successfully\n        conn.autocommit = True\n\n        # Create a cursor object\n        cur = conn.cursor()\n        \n        # Iterate through the SQL scripts to create tables\n        for table, script in sql_scripts.items():\n            try:\n                # Open the specified SQL script in read mode\n                with open(script, 'r') as file:\n                    sql_command = file.read()\n\n                # Execute the SQL command\n                cur.execute(sql_command)\n                print(f\"Table '{table}' created successfully.\")\n                \n            except psycopg2.errors.DuplicateTable:\n                print(f\"Table '{table}' already exists.\")\n            except Exception as e:\n                print(f\"Error occurred while processing the script for table '{table}': {e}\")\n\n    except Exception as e:\n        print(f\"Error occurred while creating tables: {e}\")\n    finally:\n        # Close the cursor if it was created\n        if cur:\n            cur.close()\n        # Release the connection back to the pool\n        if conn:\n            connection_pool.putconn(conn)\n            ",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def create_database(host, port, user, password, database):\n    \"\"\"Create a PostgreSQL database.\n\n    Args:\n        host (str): The host where the PostgreSQL server is running.\n        port (int): The port number for the PostgreSQL server.\n        user (str): The PostgreSQL username.\n        password (str): The PostgreSQL password.\n        database (str): The name of the database to create.\n\n    Prints:\n        A success message if the database is created, or an error message if it already exists,\n        if a connection to the server fails, or if another error occurs during database creation.\n    \"\"\"\n  \n    conn = None\n    try:\n        # Connect to PostgreSQL server without specifying a database\n        conn = psycopg2.connect(\n            host=host,\n            port=port,\n            user=user,\n            password=password\n        )\n        conn.autocommit = True  # Enable autocommit mode to allow CREATE DATABASE\n\n        # Create a cursor\n        cur = conn.cursor()\n\n        # Create the database\n        cur.execute(f\"CREATE DATABASE {database};\")\n        print(f\"Database '{database}' created successfully.\")\n        \n    except psycopg2.errors.DuplicateDatabase:\n        print(f\"Database '{database}' already exists.\")\n    except Exception as e:\n        print(f\"Error occurred while creating the database: {e}\")\n    finally:\n        # Close cursor and connection\n        if cur:\n            cur.close()\n        if conn:\n            conn.close()  # Close connection to the serv",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def crawl(url, folder, max_pages):\n    \"\"\"Crawl a webpage to download HTML content and extract data.\n\n    This function checks whether the maximum number of pages to crawl\n    has been reached. If not, it retrieves the HTML content from the\n    specified URL, saves it to a file, extracts additional links for\n    further crawling, and gathers webpage data.\n\n    Args:\n        url (str): The URL of the webpage to crawl.\n        folder (str): The directory where HTML files will be saved.\n        MAX_PAGES (int): The maximum number of pages to crawl.\n    \"\"\"\n    \n    global crawled_count, data_batch, url_start  # Access global variables\n\n    # Check if we have crawled enough pages\n    with crawled_lock:\n        if crawled_count >= MAX_PAGES:\n            return\n\n    try:\n        # Each thread creates its own WebDriver instance\n        driver = webdriver.Chrome()\n        driver.get(url)\n\n        # Save the HTML content to a file\n        with crawled_lock:\n            filename = os.path.join(folder, f\"webpage_{crawled_count + 1}.html\")\n            crawled_count += 1\n            \n        with open(filename, 'w', encoding='utf-8') as file:\n            file.write(driver.page_source)\n\n        # Parse HTML to find links for further crawling\n        soup = BeautifulSoup(driver.page_source, 'html.parser')\n        links = soup.find_all('a', href=True)  # Find all anchor tags with href attributes\n        \n        # Extract article data from webpage\n        data = extract_webpage_data(soup)  # Call extract_webpage_data\n        data.append(url)  # Append webpage URL to data list\n        data_batch.append(data)  # Append data list to data_batch list\n\n        # Add new links to the list of URLs to crawl\n        with crawled_lock:\n            for link in links:\n                new_url = link['href']\n                if new_url.startswith(tuple(url_start)): # Ensure that new url starts with all possible categories\n                    urls_to_crawl.append(new_url)  # Add the link to the list of URLs\n\n    except Exception as e:\n        print(f\"Error crawling {url}: {e}\")\n    \n    finally:\n        driver.quit()  # Close the browser after each crawl\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def worker(folder, MAX_PAGES):\n    \"\"\"Process URLs from the crawl queue in a worker thread.\n\n    This function retrieves the next URL from the shared `urls_to_crawl` list\n    and calls the `crawl` function to process it. It runs until either there\n    are no more URLs or the maximum number of pages has been crawled.\n\n    Args:\n        folder (str): The folder where HTML files will be saved.\n        MAX_PAGES (int): The maximum number of pages to crawl.\n\n    Returns:\n        None\n    \"\"\"\n    while True:\n        with crawled_lock:\n            if len(urls_to_crawl) == 0 or crawled_count >= MAX_PAGES:\n                break\n            url = urls_to_crawl.pop(0)  # Get the next URL from the list\n\n        crawl(url, folder, MAX_PAGES)  # Crawl the page and download content\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def run_parallel_crawling(folder, MAX_PAGES, NUM_WORKERS=5):\n    \"\"\"Execute parallel crawling using multiple worker threads.\n\n    This function creates a specified folder for saving crawled pages and\n    initiates parallel crawling by launching multiple worker threads.\n\n    Args:\n        folder (str): The folder to save crawled pages.\n        MAX_PAGES (int): The maximum number of pages to crawl.\n        NUM_WORKERS (int, optional): The number of parallel workers. Default is 5.\n\n    Returns:\n        None\n    \"\"\"\n    global start_url # Call global variable\n    \n    # Create folder for saving crawled pages\n    os.makedirs(folder, exist_ok=True)\n    \n    # Use joblib's Parallel to run multiple threads\n    print(f\"Starting the web scraping process for url: '{start_url}'. Collecting data... Please wait.\")\n    Parallel(n_jobs = NUM_WORKERS, backend = \"threading\")(\n        delayed(worker)(folder, MAX_PAGES) for _ in range(NUM_WORKERS)\n    )\n\n    ",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "def main():\n    \"\"\"Main function to initialize the connection pool, create the database and tables,\n    run the web crawler, and add collected data to the database.\n\n    This function orchestrates the entire workflow by initializing the connection\n    pool, creating the necessary database and tables, performing the web crawling\n    operation, and finally inserting the gathered data into the appropriate tables\n    in the PostgreSQL database.\n    \"\"\"\n    \n    global folder, MAX_PAGES, NUM_WORKERS\n\n    # Call the create_database function\n    create_database(host, port, user, password, db_name)\n    \n    # Initialise connection pool to Postgres server\n    connection_pool = create_connection_pool(minconn, maxconn, host, port, user, password, db_name)\n    \n    # Call the create_tables function\n    create_tables(connection_pool, sql_scripts)\n \n    # Start web crawling\n    run_parallel_crawling(folder, MAX_PAGES, NUM_WORKERS)\n    print(\"Web scraping operation completed successfully. Data has been gathered.\")\n    \n    # Add the batch of collected data from webpages to the tables in the database\n    add_data_to_tables(connection_pool, data_batch)\n\n    # Close the connection pool after all operations are complete\n    connection_pool.closeall()\n    print(\"All operations completed successfully. The program has finished running.\")\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "##### Execute web crawling and data extraction program",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "if __name__ == \"__main__\":\n    main()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}